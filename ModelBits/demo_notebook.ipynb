{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from get_dataset import Get_Dataset\n",
    "from eval_metrics import eval_run\n",
    "path = os.path.dirname(os.path.abspath('demo_notebook.ipynb'))\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structure\n",
    "\n",
    "The ClaimBuster dataset consists of over 23000 sentences, labelled as NFS, UFS, CFS. Here, we give a brief preview of how the data is structured and how it was processed. The dataset is composed of three parts: all_sentences (containing every sentence from every debate from 1960 to 2016, unlabelled), crowdsourced (containing all crowdsourced labelled examples) and groundtruth (containing all examples labelled by dataset authors).\n",
    "\n",
    "Below we show some examples of the data. The \"Verdict\" column gives the classification of the claim, with -1 = NFS, 0 = UFS, 1 = CFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Speaker_title</th>\n",
       "      <th>Speaker_party</th>\n",
       "      <th>File_id</th>\n",
       "      <th>Length</th>\n",
       "      <th>Line_number</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>You know, I saw a movie - \"Crocodile Dundee.\"</td>\n",
       "      <td>George Bush</td>\n",
       "      <td>Vice President</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>1988-09-25.txt</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>We're consuming 50 percent of the world's cocaine.</td>\n",
       "      <td>Michael Dukakis</td>\n",
       "      <td>Governor</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>1988-09-25.txt</td>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "      <td>-0.740979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>That answer was about as clear as Boston harbor.</td>\n",
       "      <td>George Bush</td>\n",
       "      <td>Vice President</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>1988-09-25.txt</td>\n",
       "      <td>9</td>\n",
       "      <td>129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131</td>\n",
       "      <td>Let me help the governor.</td>\n",
       "      <td>George Bush</td>\n",
       "      <td>Vice President</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>1988-09-25.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>131</td>\n",
       "      <td>0.212987</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>We've run up more debt in the last eight years than under all the presidents from George Washington to Jimmy Carter combined.</td>\n",
       "      <td>Michael Dukakis</td>\n",
       "      <td>Governor</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>1988-09-25.txt</td>\n",
       "      <td>22</td>\n",
       "      <td>172</td>\n",
       "      <td>-0.268506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id  \\\n",
       "0           26   \n",
       "1           80   \n",
       "2          129   \n",
       "3          131   \n",
       "4          172   \n",
       "\n",
       "                                                                                                                            Text  \\\n",
       "0                                                                                  You know, I saw a movie - \"Crocodile Dundee.\"   \n",
       "1                                                                             We're consuming 50 percent of the world's cocaine.   \n",
       "2                                                                               That answer was about as clear as Boston harbor.   \n",
       "3                                                                                                      Let me help the governor.   \n",
       "4  We've run up more debt in the last eight years than under all the presidents from George Washington to Jimmy Carter combined.   \n",
       "\n",
       "           Speaker   Speaker_title Speaker_party         File_id  Length  \\\n",
       "0      George Bush  Vice President    REPUBLICAN  1988-09-25.txt       9   \n",
       "1  Michael Dukakis        Governor      DEMOCRAT  1988-09-25.txt       8   \n",
       "2      George Bush  Vice President    REPUBLICAN  1988-09-25.txt       9   \n",
       "3      George Bush  Vice President    REPUBLICAN  1988-09-25.txt       5   \n",
       "4  Michael Dukakis        Governor      DEMOCRAT  1988-09-25.txt      22   \n",
       "\n",
       "   Line_number  Sentiment  Verdict  \n",
       "0           26   0.000000        0  \n",
       "1           80  -0.740979        1  \n",
       "2          129   0.000000       -1  \n",
       "3          131   0.212987       -1  \n",
       "4          172  -0.268506        1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_groundtruth = pd.read_csv(str(path)+'/ClaimBuster_Datasets/datasets/groundtruth.csv')\n",
    "df_all_sentences = pd.read_csv(str(path)+'/ClaimBuster_Datasets/datasets/all_sentences.csv')\n",
    "df_crowdsourced = pd.read_csv(str(path)+'/ClaimBuster_Datasets/datasets/crowdsourced.csv')\n",
    "\n",
    "df_groundtruth.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We concatenate the groundtruth and crowdsourced CSVs into a single labelled dataset. We then tokenise each sentence in the dataset using a bert-base-cased tokeniser. Finally, the examples in the dataset are organised into a series of triples of sentences, $S_P$, $S_T$, $S_F$, each of which has an associated label for $S_T$, the target sentence we aim to classify. We reindex the labels such that 0 = NFS, 1 = UFS, 2 = CFS.\n",
    "\n",
    "For each $S_T$, the preceding and following sentences are drawn from the \"all_sentences\" dataset, so that all labelled examples have context sentences included even if these context sentences do not possess labels of their own. If the target sentence $S_T$ comes from the first or last line of a respective debate, then its preceding/following contextual sentence is left blank, as we assume independence between debates. \n",
    "\n",
    "Data was preprocessed within the \"Preprocessing.py\" file into an 80:20 train/test split, and training/test sets were saved into the \"Data\" folder. These training and test sets were kept constant throughout the model development process, in order to accurately compare the performance of our models.\n",
    "\n",
    "Shown below are example inputs that are ready to be fed into the model, having been tokenised and concatenated. The full preprocessing code can be found in the \"Preprocessing.py\" file, which processed and saved the dataset. The \"Get_Dataset()\" function extracts the data and prepares it to be fed into the model for training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[101, 1109, 4223, 2088, 117, 1103, 7228, 2088, 1104, 1103, 1244, 1311, 1138, 1309, 1151, 1167, 2407, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 4081, 1423, 1141, 1104, 1103, 7885, 12359, 1209, 22366, 1106, 1103, 1864, 1115, 25922, 1110, 1107, 1126, 3432, 1344, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1262, 25527, 117, 1107, 2538, 1104, 1103, 5910, 1104, 1103, 3331, 4813, 117, 1103, 2978, 4013, 2757, 117, 1177, 4268, 1494, 1366, 1114, 1115, 117, 1150, 2195, 109, 3102, 1550, 1121, 1103, 3331, 4813, 1149, 1104, 1103, 9455, 15906, 3098, 1113, 9468, 19878, 24262, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[101, 1124, 1169, 1294, 1251, 9107, 1119, 3349, 117, 1133, 1103, 9193, 1132, 1115, 1195, 112, 1231, 7914, 1103, 1295, 1104, 8362, 4935, 10105, 6556, 1104, 1412, 1416, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1262, 1112, 1103, 6556, 1104, 1103, 1416, 1110, 4138, 9658, 117, 5006, 1103, 1155, 24097, 1115, 1195, 1274, 112, 189, 1920, 1105, 1195, 112, 1231, 1280, 1106, 1660, 1948, 1111, 1142, 2199, 1137, 1115, 2199, 1105, 1136, 1111, 1482, 1107, 1103, 1426, 1104, 2245, 1110, 5733, 21321, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 2421, 1143, 1198, 1587, 1128, 1150, 1103, 7141, 1110, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[101, 1135, 112, 188, 1126, 2486, 146, 1221, 170, 1974, 1164, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 146, 1108, 170, 1353, 2949, 1825, 1111, 170, 1229, 1107, 1745, 2245, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1188, 1110, 1126, 3469, 1115, 112, 188, 1125, 1185, 2197, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[101, 1262, 1103, 1314, 1645, 146, 112, 173, 1176, 1106, 1474, 1110, 1142, 131, 1188, 9478, 2239, 1114, 1103, 2461, 1913, 1107, 112, 5117, 1108, 6434, 117, 1105, 1828, 119, 4100, 1189, 1146, 1111, 1122, 1114, 1210, 9712, 6824, 2758, 1279, 117, 1141, 1222, 1412, 1319, 11989, 1107, 1999, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1337, 112, 188, 1136, 1103, 1236, 1106, 1576, 1412, 2880, 2818, 117, 1259, 1835, 2597, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 2958, 117, 146, 112, 173, 1176, 1106, 3368, 1146, 1113, 1115, 1553, 117, 2140, 117, 1105, 1113, 1240, 5767, 1111, 170, 3407, 4929, 1104, 1237, 7891, 1863, 1107, 2880, 5707, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[101, 1327, 4418, 1547, 1103, 1397, 2084, 1202, 14863, 118, 1107, 1103, 1856, 1104, 1126, 14863, 118, 1126, 15299, 1216, 1112, 2743, 2977, 1137, 1103, 5953, 118, 4073, 3465, 118, 22233, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1622, 1103, 2484, 7587, 1104, 25827, 117, 148, 11680, 22680, 2137, 3663, 131, 2119, 1519, 1143, 1474, 1115, 146, 1341, 1115, 1103, 2084, 5049, 1107, 170, 1295, 1104, 1472, 1877, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1752, 117, 1112, 170, 7663, 2301, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Sentences  \\\n",
       "0                          [[101, 1109, 4223, 2088, 117, 1103, 7228, 2088, 1104, 1103, 1244, 1311, 1138, 1309, 1151, 1167, 2407, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 4081, 1423, 1141, 1104, 1103, 7885, 12359, 1209, 22366, 1106, 1103, 1864, 1115, 25922, 1110, 1107, 1126, 3432, 1344, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1262, 25527, 117, 1107, 2538, 1104, 1103, 5910, 1104, 1103, 3331, 4813, 117, 1103, 2978, 4013, 2757, 117, 1177, 4268, 1494, 1366, 1114, 1115, 117, 1150, 2195, 109, 3102, 1550, 1121, 1103, 3331, 4813, 1149, 1104, 1103, 9455, 15906, 3098, 1113, 9468, 19878, 24262, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]   \n",
       "1                  [[101, 1124, 1169, 1294, 1251, 9107, 1119, 3349, 117, 1133, 1103, 9193, 1132, 1115, 1195, 112, 1231, 7914, 1103, 1295, 1104, 8362, 4935, 10105, 6556, 1104, 1412, 1416, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1262, 1112, 1103, 6556, 1104, 1103, 1416, 1110, 4138, 9658, 117, 5006, 1103, 1155, 24097, 1115, 1195, 1274, 112, 189, 1920, 1105, 1195, 112, 1231, 1280, 1106, 1660, 1948, 1111, 1142, 2199, 1137, 1115, 2199, 1105, 1136, 1111, 1482, 1107, 1103, 1426, 1104, 2245, 1110, 5733, 21321, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 2421, 1143, 1198, 1587, 1128, 1150, 1103, 7141, 1110, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]   \n",
       "2                                                                                                                                                                                 [[101, 1135, 112, 188, 1126, 2486, 146, 1221, 170, 1974, 1164, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 146, 1108, 170, 1353, 2949, 1825, 1111, 170, 1229, 1107, 1745, 2245, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1188, 1110, 1126, 3469, 1115, 112, 188, 1125, 1185, 2197, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]   \n",
       "3  [[101, 1262, 1103, 1314, 1645, 146, 112, 173, 1176, 1106, 1474, 1110, 1142, 131, 1188, 9478, 2239, 1114, 1103, 2461, 1913, 1107, 112, 5117, 1108, 6434, 117, 1105, 1828, 119, 4100, 1189, 1146, 1111, 1122, 1114, 1210, 9712, 6824, 2758, 1279, 117, 1141, 1222, 1412, 1319, 11989, 1107, 1999, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1337, 112, 188, 1136, 1103, 1236, 1106, 1576, 1412, 2880, 2818, 117, 1259, 1835, 2597, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 2958, 117, 146, 112, 173, 1176, 1106, 3368, 1146, 1113, 1115, 1553, 117, 2140, 117, 1105, 1113, 1240, 5767, 1111, 170, 3407, 4929, 1104, 1237, 7891, 1863, 1107, 2880, 5707, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]   \n",
       "4                                                                      [[101, 1327, 4418, 1547, 1103, 1397, 2084, 1202, 14863, 118, 1107, 1103, 1856, 1104, 1126, 14863, 118, 1126, 15299, 1216, 1112, 2743, 2977, 1137, 1103, 5953, 118, 4073, 3465, 118, 22233, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1622, 1103, 2484, 7587, 1104, 25827, 117, 148, 11680, 22680, 2137, 3663, 131, 2119, 1519, 1143, 1474, 1115, 146, 1341, 1115, 1103, 2084, 5049, 1107, 170, 1295, 1104, 1472, 1877, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [101, 1752, 117, 1112, 170, 7663, 2301, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]   \n",
       "\n",
       "   Labels  \n",
       "0       0  \n",
       "1       2  \n",
       "2       1  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = Get_Dataset(train=True)\n",
    "processed_examples = pd.DataFrame({'Sentences': trainset.sentences.tolist(), 'Labels': trainset.labels.tolist()})\n",
    "processed_examples.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architectures\n",
    "\n",
    "Detailed below are the model architectures for which we will investigate performance. Full code for each model can be found in the \"network.py\" file\n",
    "\n",
    "#### Network 0 (baseline):\n",
    "\n",
    "Inputs: [$S_T$]\n",
    "\n",
    "Outputs: y $\\in$ {0,1}\n",
    "\n",
    "Network with single BERT embedding layer, followed by MLP (as in the ClaimBuster Adversarial Transformer paper)\n",
    "* Embeds the target sentence using a BERT transformer.\n",
    "* Passes pooled output (CLS tokens) through a two layer MLP.\n",
    "* Outputs binary classification where 0 = NFS or UFS, 1 = CFS \n",
    "\n",
    "#### Network 1:\n",
    "\n",
    "Inputs: [$S_P$, $S_T$, $S_F$]\n",
    "\n",
    "Outputs: y $\\in$ {0,1,2}\n",
    "\n",
    "Network with 3 parallel BERT embedding layers, followed by LSTM layer and MLP layer\n",
    "* Embeds three sentences using BERT transformers.\n",
    "* Passes pooled outputs (CLS tokens) of each sentencea s a sequence to a Bi-LSTM. \n",
    "* Takes middle hidden state of Bi-LSTM and passes through a two layer MLP.\n",
    "* Outputs multiclass classification where 0 = NFS, 1 = UFS, 2 = CFS \n",
    "\n",
    "#### Network 2:\n",
    "\n",
    "Inputs: [$S_P$, $S_T$, $S_F$]\n",
    "\n",
    "Outputs: y $\\in$ {0,1,2}\n",
    "\n",
    "Network with 3 parallel BERT embedding layers, followed by attention layer and MLP layer\n",
    "* Embeds three sentences using BERT transformers.\n",
    "* Passes pooled outputs (CLS tokens) into two attention heads, computing attention between (S_T, S_P) and (S_T, S_F). \n",
    "* Concatenates outputs of attention heads and passes through a two layer MLP.\n",
    "* Outputs multiclass classification where 0 = NFS, 1 = UFS, 2 = CFS \n",
    "\n",
    "#### Network 3:\n",
    "\n",
    "Inputs: [$S_P$, $S_T$, $S_F$]\n",
    "\n",
    "Outputs: y $\\in$ {0,1,2}\n",
    "\n",
    "Network with 3 parallel BERT embedding layers, followed by attention layer and MLP layer\n",
    "* Embeds three sentences using BERT transformers.\n",
    "* Passes pooled outputs (CLS tokens) into two attention heads, computing attention between (S_T, S_P) and (S_T, S_F). \n",
    "* Concatenates outputs of attention heads AND S_T output of BERT layer and passes through a two layer MLP.\n",
    "* Outputs multiclass classification where 0 = NFS, 1 = UFS, 2 = CFS \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The models were trained using the train.py file. The models were trained for 15 epochs with a batch size of 4. A basic SGD optimiser with a learning rate of 0.001 was used, as this was found to converge better than other optimisers in early experiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "For each model, we evaluate by computing the precision, recall and F1 scores for each class on the test set, as well as overall weighted avergage P, R, F1. We also output the confusion matrices for each model. Tests for each model can be run in the cells below, with metrics printed in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell calls the eval_run function from the eval_metrics file\n",
    "Precision, Recall, F1 arrays are ordered by the respective classes in the format \"array([0,1,2])\".\n",
    "\n",
    "If you would like to view the outputs in a csv file, pass the argument \"save=True\" into the function call below.\n",
    "\"\"\"\n",
    "eval_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP00",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
